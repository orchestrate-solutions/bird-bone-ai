# Neurostructured AI Compression & Integration

> **A bird-bone inspired journey toward lightweight, multi-modal, Spirit-aligned intelligence**  
> *Project codename: **bird-bone** ‚Äî Version 0.2 (2025-06-04)*


---

## ‚ú® Vision
Turn heavyweight language/vision models into **agile, biologically patterned intelligences** that run at 20-30 % of their original compute cost while losing ‚â§ 3 % task accuracy‚Äîusing the same design wisdom God wove into hollow bird bones, synaptic pruning, and human override capacity.

---

## üöÄ Core Objectives
1. **Bird-Bone Compression (BBCF)** ‚Äì remove post-training support weights the way bone sheds growth plates.  
2. **Biophase Adaptive Pruning (BAP)** ‚Äì cluster-aware, slow-radius pruning with built-in ‚Äúself-healing‚Äù micro-tunes.  
3. **Quantization & Low-Rank Factorization** ‚Äì compress precision and re-factor matrices for 4 √ó memory cuts.  
4. **Neurostructured AI Flow (NAIF)** ‚Äì re-route tokens through convergence hubs (digital hippocampus) for unified multi-modal reasoning.  
5. **Automation Pipeline** ‚Äì YAML-driven, reproducible growth ‚Üí prune ‚Üí heal loop, outputting ready-to-serve INT4 GGUF weights.  
6. **Diff & Revert Safety Net** ‚Äì track every code, config, weight, and doc change with deterministic diffing so we can roll back *any* layer of the stack without drama.

---

## üó∫ Repository Map
| Path            | Purpose                                                                    |
| --------------- | -------------------------------------------------------------------------- |
| `/config`       | YAML / Pydantic manifests (model, pruning thresholds, quant settings).     |
| `/pipelines`    | Kedro / Airflow DAGs for growth-prune-heal cycles.                         |
| `/scripts`      | Stand-alone helpers: SparseGPT, Wanda, RigL, QLoRA merge, **`diff_utils.py`**. |
| `/diffs`        | Auto-generated `(before ‚Üî after)` patches for docs, configs, small weight deltas. |
| `/notebooks`    | Dashboards: activation heat-maps, healing curves, resource plots.          |
| `/requirements` | Living specs: **requirements.md**, **user-stories-mapping.md**.            |
| `/models`       | Versioned checkpoints, pruning masks, LoRA deltas, export GGUF.            |
| `/docs`         | Generated diagrams, architecture overviews, this README.                   |

---

## üîÄ Diff & Revert Strategy
We iterate fast and prune aggressively, so we need **granular diffing** across three artifact layers.

| Layer               | Storage & Versioning | Diff Tool                               | Roll-Back Command                    |
| ------------------- | -------------------- | --------------------------------------- | ------------------------------------ |
| **Code & Text**     | Git                  | `git diff`, `delta`, `diff-so-fancy`    | `git revert <sha>`                   |
| **Configs / YAML**  | Git-tracked          | `yq diff`, Pydantic hash                | `kedro config diff --rollback`       |
| **Weights / Masks** | Git LFS + DVC        | `dvc diff` (binary-aware)               | `dvc checkout <rev>`                 |
| **Notebooks**       | nbdime               | `nbdime diff notebook.ipynb`            | `nbdime checkout --rev=<rev>`        |

### Auto-Diff Hook
```bash
pre-commit install
# Installs:
#  ‚úì black & flake8 (code)
#  ‚úì mdformat (docs)
#  ‚úì nbdime (notebooks)
#  ‚úì custom weight-hash hook (models/*.pt, *.safetensors)
````

Every commit drops a **diff bundle** into `/diffs/<commit-sha>/` so reviewers can inspect exact parameter removals or sparsity-mask changes.

---

## ‚ö° Quick-Start (7-B Proof-of-Concept)

```bash
# 1. Clone & set up
conda create -n naif python=3.11
conda activate naif
pip install -r requirements.txt  # torch, transformers, bitsandbytes, sparsegpt, kedro, dvc, nbdime

# 2. Init diff layer
pre-commit install        # code+doc hooks
dvc init --subdir models   # binary diff control

# 3. Pull starter checkpoint (e.g., Mistral-7B)
git lfs install
dvc pull mistral7b.dvc     # or git lfs clone HF repo

# 4. Run growth-prune-heal
kedro run --params:model=mistral7b --params:target_sparsity=0.70

# 5. Export final INT4 GGUF
python scripts/export_gguf.py --ckpt best.pt --out models/mistral7b-bbcf.gguf

# 6. Review diff bundle
dvc diff HEAD~1
```

*Expect ‚âà 60-70 % resource drop with < 3 % accuracy drift ‚Äî all diff-tracked.*

---

## üîÑ Pipeline Stages

1. **Seeding** ‚Äì snapshot baseline metrics & SHA.
2. **Activation Logging** ‚Äì 2-3 epochs, store cluster stats.
3. **Pruning Wave** ‚Äì delete ‚â§ 10 % low-value nodes/cluster.
4. **Micro-Heal** ‚Äì 200-500 QLoRA gradient steps.
5. **Regrowth** ‚Äì RigL re-introduces up to 5 % connections if loss spike > Œµ.
6. **Quantize + Low-Rank Merge**.
7. **Version Bump** ‚Äì tag commit; push diff bundle via DVC.

---

## üìä Monitor & Guardrails

* Metrics: `ppl`, `MMLU`, `ARC`, domain-specific canaries.
* Stop-loss: pipeline auto-reverts if Œîppl > 3 % for two consecutive waves.
* Healing curves: Jupyter notebook `notebooks/healing.ipynb`; compare via nbdime.

---

## ü§ù Contributing

1. Fork ‚Üí feature branch ‚Üí PR.
2. Link each PR to a **Req ID** in `requirements.md` *and* the diff bundle ID.
3. Include before/after resource and accuracy deltas.
4. Faith & ethics: ensure changes align with our ‚Äústrength-through-purpose‚Äù ethos.

---

## üôè Acknowledgements

Open-weights communities (Mistral, Meta, Google, TII), pruning-tool authors (SparseGPT, Wanda, RigL), diff tooling (DVC, nbdime), and the insight that **design reveals intent**‚Äîfrom hollow bones to synaptic pruning to Spirit-led override.
